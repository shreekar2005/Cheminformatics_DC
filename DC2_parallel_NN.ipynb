{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreekar2005/Cheminformatics_DC/blob/main/DC2_parallel_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0IySmYN00Nx",
        "outputId": "b1846b6a-a942-49e5-a596-1a4e9c11bb69"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import io\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load data\n",
        "y_tr = pd.read_csv('drive/MyDrive/DATASET/tox21_labels_train.csv.gz', index_col=0, compression=\"gzip\")\n",
        "y_te = pd.read_csv('drive/MyDrive/DATASET/tox21_labels_test.csv.gz', index_col=0, compression=\"gzip\")\n",
        "x_tr_dense = pd.read_csv('drive/MyDrive/DATASET/tox21_dense_train.csv.gz', index_col=0, compression=\"gzip\").values\n",
        "x_te_dense = pd.read_csv('drive/MyDrive/DATASET/tox21_dense_test.csv.gz', index_col=0, compression=\"gzip\").values\n",
        "x_tr_sparse = io.mmread('drive/MyDrive/DATASET/tox21_sparse_train.mtx.gz').tocsc()\n",
        "x_te_sparse = io.mmread('drive/MyDrive/DATASET/tox21_sparse_test.mtx.gz').tocsc()\n",
        "\n",
        "# Filter sparse features\n",
        "sparse_col_idx = ((x_tr_sparse > 0).mean(0) > 0.05).A.ravel()\n",
        "x_tr = np.hstack([x_tr_dense, x_tr_sparse[:, sparse_col_idx].toarray()])\n",
        "x_te = np.hstack([x_te_dense, x_te_sparse[:, sparse_col_idx].toarray()])\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "x_tr = scaler.fit_transform(x_tr)\n",
        "x_te = scaler.transform(x_te)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_full = torch.tensor(x_tr, dtype=torch.float32)\n",
        "X_test_full = torch.tensor(x_te, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "o-K_5cmmh_DS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define simple binary classifier NN\n",
        "class BinaryNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(BinaryNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "EWx5LIqeE4vP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [None] * len(y_tr.columns)  # Create list to store models\n",
        "\n",
        "# Training loop for each assay\n",
        "for i, target in enumerate(y_tr.columns):\n",
        "    print(f\"\\n{'='*60}\\nTraining NN for assay {i+1}: {target}\")\n",
        "\n",
        "    # Get valid rows for training\n",
        "    train_mask = np.isfinite(y_tr[target].values)\n",
        "    X_train = X_train_full[train_mask]\n",
        "    Y_train = torch.tensor(y_tr[target][train_mask].values.reshape(-1, 1), dtype=torch.float32)\n",
        "    print(Y_train.shape)\n",
        "    # Initialize model, loss, optimizer\n",
        "    model = BinaryNN(X_train.shape[1])\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train model\n",
        "    epochs = 80\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, Y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Store model after training\n",
        "    models[i] = model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcWnwHQ05ax7",
        "outputId": "ec60a846-62b2-475b-9a96-e93555801d45"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training NN for assay 1: NR.AhR\n",
            "torch.Size([8441, 1])\n",
            "Epoch 5/80 - Loss: 0.4403\n",
            "Epoch 10/80 - Loss: 0.2940\n",
            "Epoch 15/80 - Loss: 0.2645\n",
            "Epoch 20/80 - Loss: 0.2418\n",
            "Epoch 25/80 - Loss: 0.2269\n",
            "Epoch 30/80 - Loss: 0.2172\n",
            "Epoch 35/80 - Loss: 0.2063\n",
            "Epoch 40/80 - Loss: 0.1982\n",
            "Epoch 45/80 - Loss: 0.1915\n",
            "Epoch 50/80 - Loss: 0.1840\n",
            "Epoch 55/80 - Loss: 0.1787\n",
            "Epoch 60/80 - Loss: 0.1712\n",
            "Epoch 65/80 - Loss: 0.1663\n",
            "Epoch 70/80 - Loss: 0.1575\n",
            "Epoch 75/80 - Loss: 0.1521\n",
            "Epoch 80/80 - Loss: 0.1457\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 2: NR.AR\n",
            "torch.Size([9654, 1])\n",
            "Epoch 5/80 - Loss: 0.2968\n",
            "Epoch 10/80 - Loss: 0.1385\n",
            "Epoch 15/80 - Loss: 0.1365\n",
            "Epoch 20/80 - Loss: 0.1310\n",
            "Epoch 25/80 - Loss: 0.1187\n",
            "Epoch 30/80 - Loss: 0.1094\n",
            "Epoch 35/80 - Loss: 0.1012\n",
            "Epoch 40/80 - Loss: 0.0933\n",
            "Epoch 45/80 - Loss: 0.0884\n",
            "Epoch 50/80 - Loss: 0.0855\n",
            "Epoch 55/80 - Loss: 0.0822\n",
            "Epoch 60/80 - Loss: 0.0791\n",
            "Epoch 65/80 - Loss: 0.0767\n",
            "Epoch 70/80 - Loss: 0.0741\n",
            "Epoch 75/80 - Loss: 0.0707\n",
            "Epoch 80/80 - Loss: 0.0683\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 3: NR.AR.LBD\n",
            "torch.Size([8852, 1])\n",
            "Epoch 5/80 - Loss: 0.3211\n",
            "Epoch 10/80 - Loss: 0.1357\n",
            "Epoch 15/80 - Loss: 0.1269\n",
            "Epoch 20/80 - Loss: 0.1236\n",
            "Epoch 25/80 - Loss: 0.1109\n",
            "Epoch 30/80 - Loss: 0.0987\n",
            "Epoch 35/80 - Loss: 0.0837\n",
            "Epoch 40/80 - Loss: 0.0765\n",
            "Epoch 45/80 - Loss: 0.0711\n",
            "Epoch 50/80 - Loss: 0.0674\n",
            "Epoch 55/80 - Loss: 0.0649\n",
            "Epoch 60/80 - Loss: 0.0607\n",
            "Epoch 65/80 - Loss: 0.0570\n",
            "Epoch 70/80 - Loss: 0.0535\n",
            "Epoch 75/80 - Loss: 0.0520\n",
            "Epoch 80/80 - Loss: 0.0485\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 4: NR.Aromatase\n",
            "torch.Size([7440, 1])\n",
            "Epoch 5/80 - Loss: 0.4485\n",
            "Epoch 10/80 - Loss: 0.2085\n",
            "Epoch 15/80 - Loss: 0.1776\n",
            "Epoch 20/80 - Loss: 0.1680\n",
            "Epoch 25/80 - Loss: 0.1634\n",
            "Epoch 30/80 - Loss: 0.1484\n",
            "Epoch 35/80 - Loss: 0.1409\n",
            "Epoch 40/80 - Loss: 0.1307\n",
            "Epoch 45/80 - Loss: 0.1255\n",
            "Epoch 50/80 - Loss: 0.1204\n",
            "Epoch 55/80 - Loss: 0.1157\n",
            "Epoch 60/80 - Loss: 0.1111\n",
            "Epoch 65/80 - Loss: 0.1051\n",
            "Epoch 70/80 - Loss: 0.1013\n",
            "Epoch 75/80 - Loss: 0.0987\n",
            "Epoch 80/80 - Loss: 0.0951\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 5: NR.ER\n",
            "torch.Size([7962, 1])\n",
            "Epoch 5/80 - Loss: 0.3800\n",
            "Epoch 10/80 - Loss: 0.3503\n",
            "Epoch 15/80 - Loss: 0.3190\n",
            "Epoch 20/80 - Loss: 0.2975\n",
            "Epoch 25/80 - Loss: 0.2883\n",
            "Epoch 30/80 - Loss: 0.2783\n",
            "Epoch 35/80 - Loss: 0.2670\n",
            "Epoch 40/80 - Loss: 0.2596\n",
            "Epoch 45/80 - Loss: 0.2542\n",
            "Epoch 50/80 - Loss: 0.2464\n",
            "Epoch 55/80 - Loss: 0.2392\n",
            "Epoch 60/80 - Loss: 0.2349\n",
            "Epoch 65/80 - Loss: 0.2279\n",
            "Epoch 70/80 - Loss: 0.2198\n",
            "Epoch 75/80 - Loss: 0.2131\n",
            "Epoch 80/80 - Loss: 0.2074\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 6: NR.ER.LBD\n",
            "torch.Size([9040, 1])\n",
            "Epoch 5/80 - Loss: 0.2692\n",
            "Epoch 10/80 - Loss: 0.1882\n",
            "Epoch 15/80 - Loss: 0.1796\n",
            "Epoch 20/80 - Loss: 0.1603\n",
            "Epoch 25/80 - Loss: 0.1448\n",
            "Epoch 30/80 - Loss: 0.1351\n",
            "Epoch 35/80 - Loss: 0.1271\n",
            "Epoch 40/80 - Loss: 0.1205\n",
            "Epoch 45/80 - Loss: 0.1139\n",
            "Epoch 50/80 - Loss: 0.1089\n",
            "Epoch 55/80 - Loss: 0.1023\n",
            "Epoch 60/80 - Loss: 0.0993\n",
            "Epoch 65/80 - Loss: 0.0940\n",
            "Epoch 70/80 - Loss: 0.0888\n",
            "Epoch 75/80 - Loss: 0.0833\n",
            "Epoch 80/80 - Loss: 0.0803\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 7: NR.PPAR.gamma\n",
            "torch.Size([8451, 1])\n",
            "Epoch 5/80 - Loss: 0.4612\n",
            "Epoch 10/80 - Loss: 0.1993\n",
            "Epoch 15/80 - Loss: 0.1506\n",
            "Epoch 20/80 - Loss: 0.1548\n",
            "Epoch 25/80 - Loss: 0.1435\n",
            "Epoch 30/80 - Loss: 0.1218\n",
            "Epoch 35/80 - Loss: 0.1080\n",
            "Epoch 40/80 - Loss: 0.0986\n",
            "Epoch 45/80 - Loss: 0.0937\n",
            "Epoch 50/80 - Loss: 0.0900\n",
            "Epoch 55/80 - Loss: 0.0848\n",
            "Epoch 60/80 - Loss: 0.0807\n",
            "Epoch 65/80 - Loss: 0.0774\n",
            "Epoch 70/80 - Loss: 0.0746\n",
            "Epoch 75/80 - Loss: 0.0702\n",
            "Epoch 80/80 - Loss: 0.0666\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 8: SR.ARE\n",
            "torch.Size([7401, 1])\n",
            "Epoch 5/80 - Loss: 0.4088\n",
            "Epoch 10/80 - Loss: 0.3655\n",
            "Epoch 15/80 - Loss: 0.3427\n",
            "Epoch 20/80 - Loss: 0.3280\n",
            "Epoch 25/80 - Loss: 0.3113\n",
            "Epoch 30/80 - Loss: 0.2971\n",
            "Epoch 35/80 - Loss: 0.2826\n",
            "Epoch 40/80 - Loss: 0.2683\n",
            "Epoch 45/80 - Loss: 0.2557\n",
            "Epoch 50/80 - Loss: 0.2423\n",
            "Epoch 55/80 - Loss: 0.2266\n",
            "Epoch 60/80 - Loss: 0.2094\n",
            "Epoch 65/80 - Loss: 0.1994\n",
            "Epoch 70/80 - Loss: 0.1824\n",
            "Epoch 75/80 - Loss: 0.1715\n",
            "Epoch 80/80 - Loss: 0.1575\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 9: SR.ATAD5\n",
            "torch.Size([9363, 1])\n",
            "Epoch 5/80 - Loss: 0.3689\n",
            "Epoch 10/80 - Loss: 0.1863\n",
            "Epoch 15/80 - Loss: 0.1936\n",
            "Epoch 20/80 - Loss: 0.1701\n",
            "Epoch 25/80 - Loss: 0.1446\n",
            "Epoch 30/80 - Loss: 0.1317\n",
            "Epoch 35/80 - Loss: 0.1244\n",
            "Epoch 40/80 - Loss: 0.1192\n",
            "Epoch 45/80 - Loss: 0.1135\n",
            "Epoch 50/80 - Loss: 0.1072\n",
            "Epoch 55/80 - Loss: 0.1020\n",
            "Epoch 60/80 - Loss: 0.0989\n",
            "Epoch 65/80 - Loss: 0.0934\n",
            "Epoch 70/80 - Loss: 0.0901\n",
            "Epoch 75/80 - Loss: 0.0855\n",
            "Epoch 80/80 - Loss: 0.0810\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 10: SR.HSE\n",
            "torch.Size([8417, 1])\n",
            "Epoch 5/80 - Loss: 0.3701\n",
            "Epoch 10/80 - Loss: 0.2235\n",
            "Epoch 15/80 - Loss: 0.2354\n",
            "Epoch 20/80 - Loss: 0.1938\n",
            "Epoch 25/80 - Loss: 0.1663\n",
            "Epoch 30/80 - Loss: 0.1644\n",
            "Epoch 35/80 - Loss: 0.1517\n",
            "Epoch 40/80 - Loss: 0.1477\n",
            "Epoch 45/80 - Loss: 0.1424\n",
            "Epoch 50/80 - Loss: 0.1351\n",
            "Epoch 55/80 - Loss: 0.1292\n",
            "Epoch 60/80 - Loss: 0.1256\n",
            "Epoch 65/80 - Loss: 0.1226\n",
            "Epoch 70/80 - Loss: 0.1170\n",
            "Epoch 75/80 - Loss: 0.1134\n",
            "Epoch 80/80 - Loss: 0.1074\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 11: SR.MMP\n",
            "torch.Size([7558, 1])\n",
            "Epoch 5/80 - Loss: 0.3856\n",
            "Epoch 10/80 - Loss: 0.3268\n",
            "Epoch 15/80 - Loss: 0.2829\n",
            "Epoch 20/80 - Loss: 0.2621\n",
            "Epoch 25/80 - Loss: 0.2431\n",
            "Epoch 30/80 - Loss: 0.2254\n",
            "Epoch 35/80 - Loss: 0.2138\n",
            "Epoch 40/80 - Loss: 0.2006\n",
            "Epoch 45/80 - Loss: 0.1879\n",
            "Epoch 50/80 - Loss: 0.1771\n",
            "Epoch 55/80 - Loss: 0.1656\n",
            "Epoch 60/80 - Loss: 0.1566\n",
            "Epoch 65/80 - Loss: 0.1447\n",
            "Epoch 70/80 - Loss: 0.1332\n",
            "Epoch 75/80 - Loss: 0.1229\n",
            "Epoch 80/80 - Loss: 0.1142\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 12: SR.p53\n",
            "torch.Size([8903, 1])\n",
            "Epoch 5/80 - Loss: 0.3741\n",
            "Epoch 10/80 - Loss: 0.2364\n",
            "Epoch 15/80 - Loss: 0.2236\n",
            "Epoch 20/80 - Loss: 0.2054\n",
            "Epoch 25/80 - Loss: 0.1862\n",
            "Epoch 30/80 - Loss: 0.1737\n",
            "Epoch 35/80 - Loss: 0.1672\n",
            "Epoch 40/80 - Loss: 0.1581\n",
            "Epoch 45/80 - Loss: 0.1501\n",
            "Epoch 50/80 - Loss: 0.1444\n",
            "Epoch 55/80 - Loss: 0.1395\n",
            "Epoch 60/80 - Loss: 0.1316\n",
            "Epoch 65/80 - Loss: 0.1265\n",
            "Epoch 70/80 - Loss: 0.1195\n",
            "Epoch 75/80 - Loss: 0.1129\n",
            "Epoch 80/80 - Loss: 0.1058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tabulate import tabulate\n",
        "\n",
        "# Evaluation Loop for each assay\n",
        "threshold = 0.5\n",
        "\n",
        "results_table = []\n",
        "macro_precisions = []\n",
        "macro_recalls = []\n",
        "macro_f1s = []\n",
        "weighted_precisions = []\n",
        "weighted_recalls = []\n",
        "weighted_f1s = []\n",
        "total_support = 0\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Final Classification Report for Selected Assays (Class 1 only):\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for i, target in enumerate(y_te.columns):\n",
        "    # Get valid test rows\n",
        "    test_mask = np.isfinite(y_te[target].values)\n",
        "    X_test = X_test_full[test_mask]\n",
        "    Y_test = y_te[target][test_mask].values\n",
        "\n",
        "    model = models[i]\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test).squeeze()\n",
        "        probs = torch.sigmoid(logits).numpy()\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    # if np.sum(preds) == 0 and np.sum(Y_test) == 0:\n",
        "    #     continue\n",
        "    # elif np.sum(preds) == 0 or np.sum(Y_test) == 0:\n",
        "    #     continue\n",
        "\n",
        "    # Confusion matrix\n",
        "    tn, fp, fn, tp = confusion_matrix(Y_test, preds).ravel()\n",
        "    support_1 = fn + tp\n",
        "    support_total = tn + fp + fn + tp\n",
        "\n",
        "    # Metrics for class 1\n",
        "    precision_1 = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "    recall_1 = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "    f1_1 = (2 * precision_1 * recall_1) / (precision_1 + recall_1) if (precision_1 + recall_1) > 0 else 0.0\n",
        "\n",
        "    results_table.append([\n",
        "        f\"Assay {i+1}: {target}\", f\"{precision_1:.4f}\", f\"{recall_1:.4f}\", f\"{f1_1:.4f}\", support_total,\n",
        "        f\"{tn}\", f\"{fp}\", f\"{fn}\", f\"{tp}\"\n",
        "    ])\n",
        "\n",
        "    # Accumulate macro and weighted scores\n",
        "    macro_precisions.append(precision_1)\n",
        "    macro_recalls.append(recall_1)\n",
        "    macro_f1s.append(f1_1)\n",
        "\n",
        "    weighted_precisions.append(precision_1 * support_1)\n",
        "    weighted_recalls.append(recall_1 * support_1)\n",
        "    weighted_f1s.append(f1_1 * support_1)\n",
        "    total_support += support_1\n",
        "\n",
        "# Print results in table\n",
        "headers = [\"Assay\", \"Precision\", \"Recall\", \"F1-Score\", \"Support\", \"TN\", \"FP\", \"FN\", \"TP\"]\n",
        "print(tabulate(results_table, headers=headers, tablefmt=\"fancy_grid\"))\n",
        "\n",
        "# Print final averages\n",
        "summary_table = [[\n",
        "    \"Macro Avg\", f\"{np.mean(macro_precisions):.6f}\", f\"{np.mean(macro_recalls):.6f}\",\n",
        "    f\"{np.mean(macro_f1s):.6f}\"\n",
        "], [\n",
        "    \"Weighted Avg\", f\"{np.sum(weighted_precisions)/total_support:.6f}\",\n",
        "    f\"{np.sum(weighted_recalls)/total_support:.6f}\",\n",
        "    f\"{np.sum(weighted_f1s)/total_support:.6f}\"\n",
        "]]\n",
        "print(\"\\n\" + tabulate(summary_table, headers=[\"Metric\", \"Precision\", \"Recall\", \"F1-Score\"], tablefmt=\"fancy_grid\"))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub7mZEE5BtVT",
        "outputId": "281847a8-1370-430c-f923-c928cc11e13b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------------------\n",
            "Final Classification Report for Selected Assays (Class 1 only):\n",
            "------------------------------------------------------------\n",
            "╒════════════════════════╤═════════════╤══════════╤════════════╤═══════════╤══════╤══════╤══════╤══════╕\n",
            "│ Assay                  │   Precision │   Recall │   F1-Score │   Support │   TN │   FP │   FN │   TP │\n",
            "╞════════════════════════╪═════════════╪══════════╪════════════╪═══════════╪══════╪══════╪══════╪══════╡\n",
            "│ Assay 1: NR.AhR        │      0.5205 │   0.5205 │     0.5205 │       610 │  502 │   35 │   35 │   38 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 2: NR.AR         │      0.5    │   0.1667 │     0.25   │       586 │  572 │    2 │   10 │    2 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 3: NR.AR.LBD     │      0      │   0      │     0      │       582 │  570 │    4 │    8 │    0 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 4: NR.Aromatase  │      0.6667 │   0.1026 │     0.1778 │       528 │  487 │    2 │   35 │    4 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 5: NR.ER         │      0.5    │   0.3137 │     0.3855 │       516 │  449 │   16 │   35 │   16 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 6: NR.ER.LBD     │      0.3    │   0.15   │     0.2    │       600 │  573 │    7 │   17 │    3 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 7: NR.PPAR.gamma │      0      │   0      │     0      │       605 │  573 │    1 │   31 │    0 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 8: SR.ARE        │      0.4375 │   0.3763 │     0.4046 │       555 │  417 │   45 │   58 │   35 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 9: SR.ATAD5      │      0.6667 │   0.1053 │     0.1818 │       622 │  582 │    2 │   34 │    4 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 10: SR.HSE       │      0.4545 │   0.2273 │     0.303  │       610 │  582 │    6 │   17 │    5 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 11: SR.MMP       │      0.5846 │   0.6333 │     0.608  │       543 │  456 │   27 │   22 │   38 │\n",
            "├────────────────────────┼─────────────┼──────────┼────────────┼───────────┼──────┼──────┼──────┼──────┤\n",
            "│ Assay 12: SR.p53       │      0.375  │   0.1463 │     0.2105 │       616 │  565 │   10 │   35 │    6 │\n",
            "╘════════════════════════╧═════════════╧══════════╧════════════╧═══════════╧══════╧══════╧══════╧══════╛\n",
            "\n",
            "╒══════════════╤═════════════╤══════════╤════════════╕\n",
            "│ Metric       │   Precision │   Recall │   F1-Score │\n",
            "╞══════════════╪═════════════╪══════════╪════════════╡\n",
            "│ Macro Avg    │    0.417129 │ 0.228505 │   0.270156 │\n",
            "├──────────────┼─────────────┼──────────┼────────────┤\n",
            "│ Weighted Avg │    0.467157 │ 0.309426 │   0.344085 │\n",
            "╘══════════════╧═════════════╧══════════╧════════════╛\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "# Calculate AUC for each target (assay)\n",
        "auc_values = []\n",
        "auc_nr_values = []  # Non-Relevant (NR)\n",
        "auc_sr_values = []  # Relevant (SR)\n",
        "\n",
        "for i, target in enumerate(y_te.columns):\n",
        "    # Get valid test rows\n",
        "    test_mask = np.isfinite(y_te[target].values)\n",
        "    X_test = X_test_full[test_mask]\n",
        "    Y_test = y_te[target][test_mask].values\n",
        "\n",
        "    model = models[i]\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test).squeeze()\n",
        "        probs = torch.sigmoid(logits).numpy()\n",
        "\n",
        "    # Calculate AUC for the current target\n",
        "    auc = roc_auc_score(Y_test, probs)\n",
        "    auc_values.append(auc)\n",
        "\n",
        "    # Classify the target as NR or SR based on its name\n",
        "    if \"NR\" in target.upper():  # NR (Non-Relevant)\n",
        "        auc_nr_values.append(auc)\n",
        "    elif \"SR\" in target.upper():  # SR (Relevant)\n",
        "        auc_sr_values.append(auc)\n",
        "\n",
        "# Calculate the averages\n",
        "avg_auc = np.mean(auc_values)\n",
        "avg_auc_nr = np.mean(auc_nr_values) if auc_nr_values else 0.0\n",
        "avg_auc_sr = np.mean(auc_sr_values) if auc_sr_values else 0.0\n",
        "\n",
        "# Create the AUC table including the averages at the front\n",
        "auc_table = [[f\"{avg_auc:.4f}\", f\"{avg_auc_nr:.4f}\", f\"{avg_auc_sr:.4f}\"] + [f\"{auc:.4f}\" for auc in auc_values]]\n",
        "\n",
        "# Print AUC table with additional columns in front\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"AUC for Selected Assays:\")\n",
        "print(\"-\"*60)\n",
        "headers = [\"Avg\", \"NR\", \"SR\"] + list(y_te.columns)\n",
        "print(tabulate(auc_table, headers=headers, tablefmt=\"fancy_grid\"))\n"
      ],
      "metadata": {
        "id": "yiP7WXp089Fm",
        "outputId": "1592448f-c612-4db7-a741-3c2e1edc6405",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------------------\n",
            "AUC for Selected Assays:\n",
            "------------------------------------------------------------\n",
            "╒════════╤════════╤════════╤══════════╤═════════╤═════════════╤════════════════╤═════════╤═════════════╤═════════════════╤══════════╤════════════╤══════════╤══════════╤══════════╕\n",
            "│    Avg │     NR │     SR │   NR.AhR │   NR.AR │   NR.AR.LBD │   NR.Aromatase │   NR.ER │   NR.ER.LBD │   NR.PPAR.gamma │   SR.ARE │   SR.ATAD5 │   SR.HSE │   SR.MMP │   SR.p53 │\n",
            "╞════════╪════════╪════════╪══════════╪═════════╪═════════════╪════════════════╪═════════╪═════════════╪═════════════════╪══════════╪════════════╪══════════╪══════════╪══════════╡\n",
            "│ 0.7972 │ 0.7765 │ 0.8262 │   0.8835 │  0.7173 │      0.6897 │         0.7893 │   0.764 │      0.7931 │          0.7988 │   0.7716 │     0.8347 │   0.8021 │   0.9265 │   0.7959 │\n",
            "╘════════╧════════╧════════╧══════════╧═════════╧═════════════╧════════════════╧═════════╧═════════════╧═════════════════╧══════════╧════════════╧══════════╧══════════╧══════════╛\n"
          ]
        }
      ]
    }
  ]
}