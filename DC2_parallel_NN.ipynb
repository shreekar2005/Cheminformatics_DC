{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shreekar2005/Cheminformatics_DC/blob/main/DC2_parallel_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0IySmYN00Nx",
        "outputId": "9a9f87df-9986-4210-cc4a-324087a037f3"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy import io\n",
        "from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Load data\n",
        "y_tr = pd.read_csv('drive/MyDrive/DATASET/tox21_labels_train.csv.gz', index_col=0, compression=\"gzip\")\n",
        "y_te = pd.read_csv('drive/MyDrive/DATASET/tox21_labels_test.csv.gz', index_col=0, compression=\"gzip\")\n",
        "x_tr_dense = pd.read_csv('drive/MyDrive/DATASET/tox21_dense_train.csv.gz', index_col=0, compression=\"gzip\").values\n",
        "x_te_dense = pd.read_csv('drive/MyDrive/DATASET/tox21_dense_test.csv.gz', index_col=0, compression=\"gzip\").values\n",
        "x_tr_sparse = io.mmread('drive/MyDrive/DATASET/tox21_sparse_train.mtx.gz').tocsc()\n",
        "x_te_sparse = io.mmread('drive/MyDrive/DATASET/tox21_sparse_test.mtx.gz').tocsc()\n",
        "\n",
        "# Filter sparse features\n",
        "sparse_col_idx = ((x_tr_sparse > 0).mean(0) > 0.05).A.ravel()\n",
        "x_tr = np.hstack([x_tr_dense, x_tr_sparse[:, sparse_col_idx].toarray()])\n",
        "x_te = np.hstack([x_te_dense, x_te_sparse[:, sparse_col_idx].toarray()])\n",
        "\n",
        "# Normalize\n",
        "scaler = StandardScaler()\n",
        "x_tr = scaler.fit_transform(x_tr)\n",
        "x_te = scaler.transform(x_te)\n",
        "\n",
        "# Convert to torch tensors\n",
        "X_train_full = torch.tensor(x_tr, dtype=torch.float32)\n",
        "X_test_full = torch.tensor(x_te, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "o-K_5cmmh_DS"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define simple binary classifier NN\n",
        "class BinaryNN(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(BinaryNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "EWx5LIqeE4vP"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = [None] * len(y_tr.columns)  # Create list to store models\n",
        "\n",
        "# Training loop for each assay\n",
        "for i, target in enumerate(y_tr.columns):\n",
        "    print(f\"\\n{'='*60}\\nTraining NN for assay {i+1}: {target}\")\n",
        "\n",
        "    # Get valid rows for training\n",
        "    train_mask = np.isfinite(y_tr[target].values)\n",
        "    X_train = X_train_full[train_mask]\n",
        "    Y_train = torch.tensor(y_tr[target][train_mask].values.reshape(-1, 1), dtype=torch.float32)\n",
        "\n",
        "    # Initialize model, loss, optimizer\n",
        "    model = BinaryNN(X_train.shape[1])\n",
        "    criterion = nn.BCEWithLogitsLoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    # Train model\n",
        "    epochs = 20\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, Y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (epoch+1) % 5 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Store model after training\n",
        "    models[i] = model\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcWnwHQ05ax7",
        "outputId": "6a25fcf7-4e3a-4a29-cb48-9938c0b01573"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training NN for assay 1: NR.AhR\n",
            "Epoch 5/20 - Loss: 0.3716\n",
            "Epoch 10/20 - Loss: 0.2992\n",
            "Epoch 15/20 - Loss: 0.2618\n",
            "Epoch 20/20 - Loss: 0.2377\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 2: NR.AR\n",
            "Epoch 5/20 - Loss: 0.2447\n",
            "Epoch 10/20 - Loss: 0.1381\n",
            "Epoch 15/20 - Loss: 0.1347\n",
            "Epoch 20/20 - Loss: 0.1284\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 3: NR.AR.LBD\n",
            "Epoch 5/20 - Loss: 0.3912\n",
            "Epoch 10/20 - Loss: 0.1579\n",
            "Epoch 15/20 - Loss: 0.1247\n",
            "Epoch 20/20 - Loss: 0.1217\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 4: NR.Aromatase\n",
            "Epoch 5/20 - Loss: 0.4132\n",
            "Epoch 10/20 - Loss: 0.2053\n",
            "Epoch 15/20 - Loss: 0.1823\n",
            "Epoch 20/20 - Loss: 0.1722\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 5: NR.ER\n",
            "Epoch 5/20 - Loss: 0.3855\n",
            "Epoch 10/20 - Loss: 0.3571\n",
            "Epoch 15/20 - Loss: 0.3231\n",
            "Epoch 20/20 - Loss: 0.3007\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 6: NR.ER.LBD\n",
            "Epoch 5/20 - Loss: 0.3286\n",
            "Epoch 10/20 - Loss: 0.1962\n",
            "Epoch 15/20 - Loss: 0.1915\n",
            "Epoch 20/20 - Loss: 0.1712\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 7: NR.PPAR.gamma\n",
            "Epoch 5/20 - Loss: 0.3677\n",
            "Epoch 10/20 - Loss: 0.1637\n",
            "Epoch 15/20 - Loss: 0.1483\n",
            "Epoch 20/20 - Loss: 0.1464\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 8: SR.ARE\n",
            "Epoch 5/20 - Loss: 0.4505\n",
            "Epoch 10/20 - Loss: 0.4073\n",
            "Epoch 15/20 - Loss: 0.3599\n",
            "Epoch 20/20 - Loss: 0.3420\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 9: SR.ATAD5\n",
            "Epoch 5/20 - Loss: 0.2925\n",
            "Epoch 10/20 - Loss: 0.1811\n",
            "Epoch 15/20 - Loss: 0.1804\n",
            "Epoch 20/20 - Loss: 0.1547\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 10: SR.HSE\n",
            "Epoch 5/20 - Loss: 0.2888\n",
            "Epoch 10/20 - Loss: 0.2247\n",
            "Epoch 15/20 - Loss: 0.2076\n",
            "Epoch 20/20 - Loss: 0.1735\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 11: SR.MMP\n",
            "Epoch 5/20 - Loss: 0.3727\n",
            "Epoch 10/20 - Loss: 0.3177\n",
            "Epoch 15/20 - Loss: 0.2766\n",
            "Epoch 20/20 - Loss: 0.2531\n",
            "\n",
            "============================================================\n",
            "Training NN for assay 12: SR.p53\n",
            "Epoch 5/20 - Loss: 0.4183\n",
            "Epoch 10/20 - Loss: 0.2526\n",
            "Epoch 15/20 - Loss: 0.2379\n",
            "Epoch 20/20 - Loss: 0.2129\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation Loop for each assay\n",
        "threshold = 0.5  # Default threshold; try adjusting this value for better results.\n",
        "\n",
        "print(\"\\n\" + \"-\"*60)\n",
        "print(\"Final Classification Report for Selected Assays:\")\n",
        "print(\"-\"*60)\n",
        "\n",
        "for i, target in enumerate(y_te.columns):\n",
        "    print(f\"\\nAssay {i+1}: {target}\")\n",
        "\n",
        "    # Get valid test rows\n",
        "    test_mask = np.isfinite(y_te[target].values)\n",
        "    X_test = X_test_full[test_mask]\n",
        "    Y_test = y_te[target][test_mask].values\n",
        "\n",
        "    model = models[i]\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        logits = model(X_test).squeeze()\n",
        "        probs = torch.sigmoid(logits).numpy()\n",
        "        preds = (probs >= threshold).astype(int)\n",
        "\n",
        "    # Check if there are positive predictions or labels\n",
        "    if np.sum(preds) == 0 and np.sum(Y_test) == 0:\n",
        "        print(\"Only negative samples — skipping evaluation.\")\n",
        "        continue\n",
        "    elif np.sum(preds) == 0:\n",
        "        print(\"⚠️ No positive predictions — skipping classification report.\")\n",
        "        print(\"Consider adjusting the threshold or using class weighting.\")\n",
        "        continue\n",
        "    elif np.sum(Y_test) == 0:\n",
        "        print(\"⚠️ No positive labels — skipping classification report.\")\n",
        "        continue\n",
        "\n",
        "    # Classification report\n",
        "    report = classification_report(Y_test, preds, output_dict=True)\n",
        "    print(f\"macro avg  precision: {report['macro avg']['precision']:.6f} | \"\n",
        "          f\"recall: {report['macro avg']['recall']:.6f} | \"\n",
        "          f\"f1-score: {report['macro avg']['f1-score']:.6f} | \"\n",
        "          f\"support: {report['macro avg']['support']}\")\n",
        "    print(f\"weighted avg precision: {report['weighted avg']['precision']:.6f} | \"\n",
        "          f\"recall: {report['weighted avg']['recall']:.6f} | \"\n",
        "          f\"f1-score: {report['weighted avg']['f1-score']:.6f} | \"\n",
        "          f\"support: {report['weighted avg']['support']}\")\n",
        "\n",
        "    # Confusion matrix\n",
        "    tn, fp, fn, tp = confusion_matrix(Y_test, preds).ravel()\n",
        "    print(f\"Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}\")\n",
        "\n",
        "    # Additional diagnostics for the assay if no positives are predicted\n",
        "    if np.sum(preds) == 0:\n",
        "        print(f\"Warning: No positive predictions for assay {target}.\")\n",
        "        print(f\"Positive samples in the test set: {np.sum(Y_test)}\")\n",
        "        print(f\"Predicted positives: {np.sum(preds)}\")\n",
        "        print(\"Consider lowering the classification threshold, checking class imbalance, or adjusting model training.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub7mZEE5BtVT",
        "outputId": "781b9323-5912-47ae-d96d-1964f4c9aa14"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "------------------------------------------------------------\n",
            "Final Classification Report for Selected Assays:\n",
            "------------------------------------------------------------\n",
            "\n",
            "Assay 1: NR.AhR\n",
            "macro avg  precision: 0.724744 | recall: 0.732673 | f1-score: 0.728598 | support: 610.0\n",
            "weighted avg precision: 0.885688 | recall: 0.883607 | f1-score: 0.884615 | support: 610.0\n",
            "Confusion Matrix: TN=500, FP=37, FN=34, TP=39\n",
            "\n",
            "Assay 2: NR.AR\n",
            "macro avg  precision: 0.573851 | recall: 0.537311 | f1-score: 0.548623 | support: 586.0\n",
            "weighted avg precision: 0.964358 | recall: 0.972696 | f1-score: 0.968217 | support: 586.0\n",
            "Confusion Matrix: TN=569, FP=5, FN=11, TP=1\n",
            "\n",
            "Assay 3: NR.AR.LBD\n",
            "macro avg  precision: 0.493080 | recall: 0.496516 | f1-score: 0.494792 | support: 582.0\n",
            "weighted avg precision: 0.972604 | recall: 0.979381 | f1-score: 0.975981 | support: 582.0\n",
            "Confusion Matrix: TN=570, FP=4, FN=8, TP=0\n",
            "\n",
            "Assay 4: NR.Aromatase\n",
            "⚠️ No positive predictions — skipping classification report.\n",
            "Consider adjusting the threshold or using class weighting.\n",
            "\n",
            "Assay 5: NR.ER\n",
            "macro avg  precision: 0.853017 | recall: 0.604617 | f1-score: 0.646997 | support: 516.0\n",
            "weighted avg precision: 0.907015 | recall: 0.916667 | f1-score: 0.894543 | support: 516.0\n",
            "Confusion Matrix: TN=462, FP=3, FN=40, TP=11\n",
            "\n",
            "Assay 6: NR.ER.LBD\n",
            "macro avg  precision: 0.650754 | recall: 0.523276 | f1-score: 0.534557 | support: 600.0\n",
            "weighted avg precision: 0.947013 | recall: 0.965000 | f1-score: 0.952318 | support: 600.0\n",
            "Confusion Matrix: TN=578, FP=2, FN=19, TP=1\n",
            "\n",
            "Assay 7: NR.PPAR.gamma\n",
            "⚠️ No positive predictions — skipping classification report.\n",
            "Consider adjusting the threshold or using class weighting.\n",
            "\n",
            "Assay 8: SR.ARE\n",
            "macro avg  precision: 0.651954 | recall: 0.545071 | f1-score: 0.546182 | support: 555.0\n",
            "weighted avg precision: 0.780685 | recall: 0.828829 | f1-score: 0.784302 | support: 555.0\n",
            "Confusion Matrix: TN=449, FP=13, FN=82, TP=11\n",
            "\n",
            "Assay 9: SR.ATAD5\n",
            "⚠️ No positive predictions — skipping classification report.\n",
            "Consider adjusting the threshold or using class weighting.\n",
            "\n",
            "Assay 10: SR.HSE\n",
            "⚠️ No positive predictions — skipping classification report.\n",
            "Consider adjusting the threshold or using class weighting.\n",
            "\n",
            "Assay 11: SR.MMP\n",
            "macro avg  precision: 0.713738 | recall: 0.710611 | f1-score: 0.712157 | support: 543.0\n",
            "weighted avg precision: 0.886843 | recall: 0.887661 | f1-score: 0.887247 | support: 543.0\n",
            "Confusion Matrix: TN=453, FP=30, FN=31, TP=29\n",
            "\n",
            "Assay 12: SR.p53\n",
            "macro avg  precision: 0.466667 | recall: 0.499130 | f1-score: 0.482353 | support: 616.0\n",
            "weighted avg precision: 0.871212 | recall: 0.931818 | f1-score: 0.900497 | support: 616.0\n",
            "Confusion Matrix: TN=574, FP=1, FN=41, TP=0\n"
          ]
        }
      ]
    }
  ]
}